{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Comparison Analysis\n",
    "\n",
    "This notebook compares two classification methods against human-labeled ground truth for the HBP content modernization project.\n",
    "\n",
    "## Overview\n",
    "- **Ground Truth**: Human-labeled data with Y/N flags for removal\n",
    "- **Experiment 1 (Copilot Studio)**: LLM responses with KEEP/REMOVE recommendations parsed from `CP_FULL_RESPONSE`\n",
    "- **Experiment 2 (Alternate)**: Y/N flags from another approach\n",
    "\n",
    "## Metrics Computed\n",
    "- Confusion Matrix (TP, TN, FP, FN)\n",
    "- Precision, Recall, F1 Score\n",
    "- Accuracy\n",
    "- Comparison visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    classification_report, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    accuracy_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "**Update these paths and column names to match your files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FILE PATHS - UPDATE THESE TO YOUR LOCAL FILE LOCATIONS\n",
    "# =============================================================================\n",
    "\n",
    "GROUND_TRUTH_FILE = \"ground_truth.xlsx\"  # Human-labeled ground truth\n",
    "COPILOT_EXPERIMENT_FILE = \"copilot_results.xlsx\"  # LLM output with CP_FULL_RESPONSE\n",
    "ALTERNATE_EXPERIMENT_FILE = \"alternate_results.xlsx\"  # Second experiment with Y/N\n",
    "\n",
    "# =============================================================================\n",
    "# COLUMN NAMES - UPDATE THESE TO MATCH YOUR SPREADSHEETS\n",
    "# =============================================================================\n",
    "\n",
    "# Ground truth file columns\n",
    "GT_ID_COLUMN = \"core_product_id\"  # or whatever your ID column is called\n",
    "GT_FLAG_COLUMN = \"flag for removal\"  # Y = remove, N = keep\n",
    "\n",
    "# Copilot experiment file columns\n",
    "CP_ID_COLUMN = \"core_product_id\"  # ID column in copilot results\n",
    "CP_RESPONSE_COLUMN = \"CP_FULL_RESPONSE\"  # Column with full LLM response to parse\n",
    "\n",
    "# Alternate experiment file columns  \n",
    "ALT_ID_COLUMN = \"core_product_id\"  # ID column in alternate results\n",
    "ALT_FLAG_COLUMN = \"flag for removal\"  # Y = remove, N = keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth\n",
    "print(\"Loading ground truth...\")\n",
    "df_ground_truth = pd.read_excel(GROUND_TRUTH_FILE)\n",
    "print(f\"  Loaded {len(df_ground_truth)} rows\")\n",
    "print(f\"  Columns: {list(df_ground_truth.columns)}\")\n",
    "\n",
    "# Load Copilot experiment results\n",
    "print(\"\\nLoading Copilot experiment results...\")\n",
    "df_copilot = pd.read_excel(COPILOT_EXPERIMENT_FILE)\n",
    "print(f\"  Loaded {len(df_copilot)} rows\")\n",
    "print(f\"  Columns: {list(df_copilot.columns)}\")\n",
    "\n",
    "# Load alternate experiment results\n",
    "print(\"\\nLoading alternate experiment results...\")\n",
    "df_alternate = pd.read_excel(ALTERNATE_EXPERIMENT_FILE)\n",
    "print(f\"  Loaded {len(df_alternate)} rows\")\n",
    "print(f\"  Columns: {list(df_alternate.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parse LLM Responses\n",
    "\n",
    "Extract KEEP/REMOVE from the `CP_FULL_RESPONSE` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_recommendation(response_text):\n",
    "    \"\"\"\n",
    "    Parse the LLM response to extract KEEP or REMOVE recommendation.\n",
    "    \n",
    "    Looks for patterns like:\n",
    "    - **Overall Recommendation:** KEEP\n",
    "    - **Overall Recommendation:** REMOVE\n",
    "    - Overall Recommendation: KEEP\n",
    "    - etc.\n",
    "    \n",
    "    Returns: 'KEEP', 'REMOVE', or 'UNPARSEABLE'\n",
    "    \"\"\"\n",
    "    if pd.isna(response_text) or not isinstance(response_text, str):\n",
    "        return 'UNPARSEABLE'\n",
    "    \n",
    "    # Normalize the text\n",
    "    text = response_text.upper()\n",
    "    \n",
    "    # Pattern 1: Look for \"Overall Recommendation:\" followed by KEEP or REMOVE\n",
    "    # Handle various markdown formatting (**, *, etc.)\n",
    "    pattern = r'\\*{0,2}OVERALL\\s+RECOMMENDATION:?\\*{0,2}\\s*\\*{0,2}(KEEP|REMOVE)\\*{0,2}'\n",
    "    match = re.search(pattern, text)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    # Pattern 2: Look for \"Recommendation:\" followed by KEEP or REMOVE\n",
    "    pattern2 = r'RECOMMENDATION:?\\s*\\*{0,2}(KEEP|REMOVE)\\*{0,2}'\n",
    "    match2 = re.search(pattern2, text)\n",
    "    \n",
    "    if match2:\n",
    "        return match2.group(1)\n",
    "    \n",
    "    # Pattern 3: Just look for KEEP or REMOVE at the end of the text\n",
    "    # (sometimes LLMs put the final answer at the very end)\n",
    "    if text.strip().endswith('KEEP'):\n",
    "        return 'KEEP'\n",
    "    if text.strip().endswith('REMOVE'):\n",
    "        return 'REMOVE'\n",
    "    \n",
    "    return 'UNPARSEABLE'\n",
    "\n",
    "\n",
    "# Test the parser on a sample\n",
    "test_cases = [\n",
    "    \"Based on my analysis... **Overall Recommendation:** KEEP**\",\n",
    "    \"The content is outdated. **Overall Recommendation:** REMOVE\",\n",
    "    \"Overall Recommendation: KEEP\",\n",
    "    \"This article should be flagged. Recommendation: REMOVE\",\n",
    "    \"Some text without a clear recommendation\",\n",
    "    None,\n",
    "]\n",
    "\n",
    "print(\"Testing parser:\")\n",
    "for tc in test_cases:\n",
    "    result = parse_recommendation(tc)\n",
    "    display_text = str(tc)[:60] + \"...\" if tc and len(str(tc)) > 60 else tc\n",
    "    print(f\"  '{display_text}' -> {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply parser to Copilot results\n",
    "df_copilot['parsed_recommendation'] = df_copilot[CP_RESPONSE_COLUMN].apply(parse_recommendation)\n",
    "\n",
    "# Check parsing results\n",
    "print(\"Parsing results summary:\")\n",
    "print(df_copilot['parsed_recommendation'].value_counts())\n",
    "\n",
    "# Show any unparseable rows for investigation\n",
    "unparseable = df_copilot[df_copilot['parsed_recommendation'] == 'UNPARSEABLE']\n",
    "if len(unparseable) > 0:\n",
    "    print(f\"\\nâš ï¸  {len(unparseable)} rows could not be parsed. Sample responses:\")\n",
    "    for idx, row in unparseable.head(3).iterrows():\n",
    "        print(f\"\\n  ID: {row[CP_ID_COLUMN]}\")\n",
    "        response = str(row[CP_RESPONSE_COLUMN])[:500]\n",
    "        print(f\"  Response: {response}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Standardize Labels\n",
    "\n",
    "Convert all labels to a common format:\n",
    "- `1` = REMOVE (flagged for removal)\n",
    "- `0` = KEEP (not flagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_label(value):\n",
    "    \"\"\"\n",
    "    Convert various label formats to binary:\n",
    "    - Y, REMOVE, 1, True -> 1 (remove)\n",
    "    - N, KEEP, 0, False -> 0 (keep)\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    \n",
    "    val = str(value).upper().strip()\n",
    "    \n",
    "    if val in ['Y', 'YES', 'REMOVE', '1', 'TRUE']:\n",
    "        return 1\n",
    "    elif val in ['N', 'NO', 'KEEP', '0', 'FALSE']:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# Standardize ground truth\n",
    "df_ground_truth['gt_label'] = df_ground_truth[GT_FLAG_COLUMN].apply(standardize_label)\n",
    "\n",
    "# Standardize Copilot results\n",
    "df_copilot['cp_label'] = df_copilot['parsed_recommendation'].apply(standardize_label)\n",
    "\n",
    "# Standardize alternate experiment\n",
    "df_alternate['alt_label'] = df_alternate[ALT_FLAG_COLUMN].apply(standardize_label)\n",
    "\n",
    "print(\"Ground Truth distribution:\")\n",
    "print(df_ground_truth['gt_label'].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\nCopilot predictions distribution:\")\n",
    "print(df_copilot['cp_label'].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\nAlternate experiment distribution:\")\n",
    "print(df_alternate['alt_label'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Merge Datasets\n",
    "\n",
    "Join all results on the article ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataframes for merging (select only needed columns)\n",
    "gt_for_merge = df_ground_truth[[GT_ID_COLUMN, 'gt_label']].copy()\n",
    "gt_for_merge = gt_for_merge.rename(columns={GT_ID_COLUMN: 'article_id'})\n",
    "\n",
    "cp_for_merge = df_copilot[[CP_ID_COLUMN, 'cp_label', 'parsed_recommendation']].copy()\n",
    "cp_for_merge = cp_for_merge.rename(columns={CP_ID_COLUMN: 'article_id'})\n",
    "\n",
    "alt_for_merge = df_alternate[[ALT_ID_COLUMN, 'alt_label']].copy()\n",
    "alt_for_merge = alt_for_merge.rename(columns={ALT_ID_COLUMN: 'article_id'})\n",
    "\n",
    "# Merge all datasets\n",
    "df_merged = gt_for_merge.merge(cp_for_merge, on='article_id', how='outer')\n",
    "df_merged = df_merged.merge(alt_for_merge, on='article_id', how='outer')\n",
    "\n",
    "print(f\"Merged dataset: {len(df_merged)} rows\")\n",
    "print(f\"\\nData availability:\")\n",
    "print(f\"  - Ground truth labels: {df_merged['gt_label'].notna().sum()}\")\n",
    "print(f\"  - Copilot predictions: {df_merged['cp_label'].notna().sum()}\")\n",
    "print(f\"  - Alternate predictions: {df_merged['alt_label'].notna().sum()}\")\n",
    "\n",
    "# Show sample\n",
    "df_merged.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculate Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, method_name):\n",
    "    \"\"\"\n",
    "    Calculate and display classification metrics.\n",
    "    \n",
    "    Positive class (1) = REMOVE\n",
    "    Negative class (0) = KEEP\n",
    "    \"\"\"\n",
    "    # Filter out any NaN values\n",
    "    mask = (~pd.isna(y_true)) & (~pd.isna(y_pred))\n",
    "    y_true_clean = y_true[mask].astype(int)\n",
    "    y_pred_clean = y_pred[mask].astype(int)\n",
    "    \n",
    "    n_samples = len(y_true_clean)\n",
    "    \n",
    "    if n_samples == 0:\n",
    "        print(f\"\\nâŒ {method_name}: No valid samples to compare!\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_true_clean, y_pred_clean)\n",
    "    \n",
    "    # Extract values (handling cases where not all classes are present)\n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "    else:\n",
    "        # Handle edge case where only one class is present\n",
    "        tn = fp = fn = tp = 0\n",
    "        if 0 in y_true_clean.values and 0 in y_pred_clean.values:\n",
    "            tn = ((y_true_clean == 0) & (y_pred_clean == 0)).sum()\n",
    "        if 0 in y_true_clean.values and 1 in y_pred_clean.values:\n",
    "            fp = ((y_true_clean == 0) & (y_pred_clean == 1)).sum()\n",
    "        if 1 in y_true_clean.values and 0 in y_pred_clean.values:\n",
    "            fn = ((y_true_clean == 1) & (y_pred_clean == 0)).sum()\n",
    "        if 1 in y_true_clean.values and 1 in y_pred_clean.values:\n",
    "            tp = ((y_true_clean == 1) & (y_pred_clean == 1)).sum()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true_clean, y_pred_clean)\n",
    "    precision = precision_score(y_true_clean, y_pred_clean, zero_division=0)\n",
    "    recall = recall_score(y_true_clean, y_pred_clean, zero_division=0)\n",
    "    f1 = f1_score(y_true_clean, y_pred_clean, zero_division=0)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸ“Š {method_name.upper()} RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nSamples evaluated: {n_samples}\")\n",
    "    print(f\"\\nðŸ“‹ Confusion Matrix:\")\n",
    "    print(f\"                    Predicted\")\n",
    "    print(f\"                 KEEP    REMOVE\")\n",
    "    print(f\"  Actual KEEP     {tn:4d}     {fp:4d}   (TN, FP)\")\n",
    "    print(f\"  Actual REMOVE   {fn:4d}     {tp:4d}   (FN, TP)\")\n",
    "    print(f\"\\nðŸ“ˆ Metrics:\")\n",
    "    print(f\"  Accuracy:  {accuracy:.4f} ({accuracy*100:.1f}%)\")\n",
    "    print(f\"  Precision: {precision:.4f} (of predicted REMOVE, how many were correct)\")\n",
    "    print(f\"  Recall:    {recall:.4f} (of actual REMOVE, how many were found)\")\n",
    "    print(f\"  F1 Score:  {f1:.4f} (harmonic mean of precision and recall)\")\n",
    "    \n",
    "    # Return metrics as dict for comparison\n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'n_samples': n_samples,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for Copilot experiment\n",
    "copilot_metrics = calculate_metrics(\n",
    "    df_merged['gt_label'], \n",
    "    df_merged['cp_label'], \n",
    "    'Copilot Studio'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for Alternate experiment\n",
    "alternate_metrics = calculate_metrics(\n",
    "    df_merged['gt_label'], \n",
    "    df_merged['alt_label'], \n",
    "    'Alternate Method'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "if copilot_metrics and alternate_metrics:\n",
    "    comparison_data = {\n",
    "        'Metric': ['Samples', 'True Positives', 'True Negatives', 'False Positives', \n",
    "                   'False Negatives', 'Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "        'Copilot Studio': [\n",
    "            copilot_metrics['n_samples'],\n",
    "            copilot_metrics['TP'],\n",
    "            copilot_metrics['TN'],\n",
    "            copilot_metrics['FP'],\n",
    "            copilot_metrics['FN'],\n",
    "            f\"{copilot_metrics['accuracy']:.4f}\",\n",
    "            f\"{copilot_metrics['precision']:.4f}\",\n",
    "            f\"{copilot_metrics['recall']:.4f}\",\n",
    "            f\"{copilot_metrics['f1']:.4f}\"\n",
    "        ],\n",
    "        'Alternate Method': [\n",
    "            alternate_metrics['n_samples'],\n",
    "            alternate_metrics['TP'],\n",
    "            alternate_metrics['TN'],\n",
    "            alternate_metrics['FP'],\n",
    "            alternate_metrics['FN'],\n",
    "            f\"{alternate_metrics['accuracy']:.4f}\",\n",
    "            f\"{alternate_metrics['precision']:.4f}\",\n",
    "            f\"{alternate_metrics['recall']:.4f}\",\n",
    "            f\"{alternate_metrics['f1']:.4f}\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ“Š SIDE-BY-SIDE COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "    print(df_comparison.to_string(index=False))\n",
    "else:\n",
    "    print(\"Cannot create comparison - one or both methods have no valid data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title, ax):\n",
    "    \"\"\"Plot a confusion matrix heatmap.\"\"\"\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['KEEP', 'REMOVE'],\n",
    "                yticklabels=['KEEP', 'REMOVE'])\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "if copilot_metrics and alternate_metrics:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    plot_confusion_matrix(copilot_metrics['confusion_matrix'], \n",
    "                          'Copilot Studio', axes[0])\n",
    "    plot_confusion_matrix(alternate_metrics['confusion_matrix'], \n",
    "                          'Alternate Method', axes[1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nðŸ’¾ Saved: confusion_matrices.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparison of metrics\n",
    "if copilot_metrics and alternate_metrics:\n",
    "    metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    copilot_values = [copilot_metrics['accuracy'], copilot_metrics['precision'], \n",
    "                      copilot_metrics['recall'], copilot_metrics['f1']]\n",
    "    alternate_values = [alternate_metrics['accuracy'], alternate_metrics['precision'], \n",
    "                        alternate_metrics['recall'], alternate_metrics['f1']]\n",
    "    \n",
    "    x = np.arange(len(metrics_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bars1 = ax.bar(x - width/2, copilot_values, width, label='Copilot Studio', color='#2196F3')\n",
    "    bars2 = ax.bar(x + width/2, alternate_values, width, label='Alternate Method', color='#FF9800')\n",
    "    \n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Classification Metrics Comparison')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics_names)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('metrics_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nðŸ’¾ Saved: metrics_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Error Analysis\n",
    "\n",
    "Examine where the methods disagree with ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find disagreements\n",
    "df_analysis = df_merged.copy()\n",
    "\n",
    "# Copilot errors\n",
    "df_analysis['cp_correct'] = df_analysis['gt_label'] == df_analysis['cp_label']\n",
    "df_analysis['cp_error_type'] = np.where(\n",
    "    df_analysis['cp_label'].isna(), 'NO_PREDICTION',\n",
    "    np.where(df_analysis['gt_label'].isna(), 'NO_GROUND_TRUTH',\n",
    "    np.where(df_analysis['cp_correct'], 'CORRECT',\n",
    "    np.where((df_analysis['gt_label'] == 0) & (df_analysis['cp_label'] == 1), 'FALSE_POSITIVE',\n",
    "    np.where((df_analysis['gt_label'] == 1) & (df_analysis['cp_label'] == 0), 'FALSE_NEGATIVE',\n",
    "    'OTHER')))))\n",
    "\n",
    "# Alternate errors\n",
    "df_analysis['alt_correct'] = df_analysis['gt_label'] == df_analysis['alt_label']\n",
    "df_analysis['alt_error_type'] = np.where(\n",
    "    df_analysis['alt_label'].isna(), 'NO_PREDICTION',\n",
    "    np.where(df_analysis['gt_label'].isna(), 'NO_GROUND_TRUTH',\n",
    "    np.where(df_analysis['alt_correct'], 'CORRECT',\n",
    "    np.where((df_analysis['gt_label'] == 0) & (df_analysis['alt_label'] == 1), 'FALSE_POSITIVE',\n",
    "    np.where((df_analysis['gt_label'] == 1) & (df_analysis['alt_label'] == 0), 'FALSE_NEGATIVE',\n",
    "    'OTHER')))))\n",
    "\n",
    "print(\"Copilot Error Analysis:\")\n",
    "print(df_analysis['cp_error_type'].value_counts())\n",
    "\n",
    "print(\"\\nAlternate Method Error Analysis:\")\n",
    "print(df_analysis['alt_error_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show false positives (predicted REMOVE but should KEEP)\n",
    "print(\"\\nðŸ”´ COPILOT FALSE POSITIVES (predicted REMOVE, actual KEEP):\")\n",
    "cp_fp = df_analysis[df_analysis['cp_error_type'] == 'FALSE_POSITIVE']\n",
    "print(f\"Count: {len(cp_fp)}\")\n",
    "if len(cp_fp) > 0:\n",
    "    print(cp_fp[['article_id', 'gt_label', 'cp_label', 'parsed_recommendation']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show false negatives (predicted KEEP but should REMOVE)\n",
    "print(\"\\nðŸ”´ COPILOT FALSE NEGATIVES (predicted KEEP, actual REMOVE):\")\n",
    "cp_fn = df_analysis[df_analysis['cp_error_type'] == 'FALSE_NEGATIVE']\n",
    "print(f\"Count: {len(cp_fn)}\")\n",
    "if len(cp_fn) > 0:\n",
    "    print(cp_fn[['article_id', 'gt_label', 'cp_label', 'parsed_recommendation']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export full analysis to Excel\n",
    "output_file = 'classification_comparison_results.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    # Summary sheet\n",
    "    if copilot_metrics and alternate_metrics:\n",
    "        df_comparison.to_excel(writer, sheet_name='Summary', index=False)\n",
    "    \n",
    "    # Full merged data with analysis\n",
    "    df_analysis.to_excel(writer, sheet_name='Full Analysis', index=False)\n",
    "    \n",
    "    # Copilot errors\n",
    "    cp_errors = df_analysis[df_analysis['cp_error_type'].isin(['FALSE_POSITIVE', 'FALSE_NEGATIVE'])]\n",
    "    cp_errors.to_excel(writer, sheet_name='Copilot Errors', index=False)\n",
    "    \n",
    "    # Alternate errors\n",
    "    alt_errors = df_analysis[df_analysis['alt_error_type'].isin(['FALSE_POSITIVE', 'FALSE_NEGATIVE'])]\n",
    "    alt_errors.to_excel(writer, sheet_name='Alternate Errors', index=False)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Results exported to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if copilot_metrics and alternate_metrics:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ“‹ SUMMARY & RECOMMENDATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Determine winner for each metric\n",
    "    metrics_to_compare = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    \n",
    "    print(\"\\nðŸ† Best Performer by Metric:\")\n",
    "    for metric in metrics_to_compare:\n",
    "        cp_val = copilot_metrics[metric]\n",
    "        alt_val = alternate_metrics[metric]\n",
    "        if cp_val > alt_val:\n",
    "            winner = \"Copilot Studio\"\n",
    "            diff = cp_val - alt_val\n",
    "        elif alt_val > cp_val:\n",
    "            winner = \"Alternate Method\"\n",
    "            diff = alt_val - cp_val\n",
    "        else:\n",
    "            winner = \"TIE\"\n",
    "            diff = 0\n",
    "        print(f\"  {metric.capitalize():12s}: {winner} (+{diff:.4f})\")\n",
    "    \n",
    "    # Overall recommendation based on F1 (balanced metric)\n",
    "    print(\"\\nðŸ“Œ Overall Recommendation:\")\n",
    "    if copilot_metrics['f1'] > alternate_metrics['f1']:\n",
    "        print(\"  â†’ Copilot Studio shows better overall performance (higher F1)\")\n",
    "    elif alternate_metrics['f1'] > copilot_metrics['f1']:\n",
    "        print(\"  â†’ Alternate Method shows better overall performance (higher F1)\")\n",
    "    else:\n",
    "        print(\"  â†’ Both methods perform equally (same F1)\")\n",
    "    \n",
    "    # Note about recall importance for this use case\n",
    "    print(\"\\nðŸ’¡ Note for HBP use case:\")\n",
    "    print(\"  - High RECALL is important if you want to catch most outdated content\")\n",
    "    print(\"    (minimize false negatives - items that should be removed but aren't flagged)\")\n",
    "    print(\"  - High PRECISION is important if you want to minimize manual review workload\")\n",
    "    print(\"    (minimize false positives - items flagged that shouldn't be)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
